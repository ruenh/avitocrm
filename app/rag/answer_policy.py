"""Answer policy with anti-hallucination contract for RAG responses."""

import logging
from typing import Literal, Optional

import google.generativeai as genai
from pydantic import BaseModel

from app.models.domain import StoredMessage
from app.rag.retrieval import RetrievalResult

logger = logging.getLogger(__name__)


class AnswerResult(BaseModel):
    """Result of answer generation."""

    answer: str
    found_status: Literal["FOUND", "NOT_FOUND", "ESCALATION"]
    sources: list[str]
    is_escalation: bool = False


class AnswerPolicy:
    """Policy for generating answers with anti-hallucination contract.
    
    Key principles:
    1. Never fabricate information about price, specs, availability
    2. Only use information from File Search citations
    3. Use fallback message when no relevant results found
    4. Handle escalation requests to human manager
    """

    FALLBACK_MESSAGE = (
        "ðŸ¤–: Ð² Ð¼Ð¾ÐµÐ¹ Ð±Ð°Ð·Ðµ Ð½ÐµÑ‚ Ð½ÑƒÐ¶Ð½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ñ‚Ð²Ð¾ÐµÐ¼Ñƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑƒ, "
        "Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð·Ð°Ð´Ð°Ñ‚ÑŒ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ðµ Ð¸Ð»Ð¸ Ð¼Ð½Ðµ Ð²Ñ‹Ð·Ð²Ð°Ñ‚ÑŒ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°?"
    )

    ESCALATION_KEYWORDS = [
        "Ð²Ñ‹Ð·Ð¾Ð²Ð¸ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°",
        "Ð¿Ð¾Ð·Ð¾Ð²Ð¸ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°", 
        "Ð¿Ð¾Ð·Ð¾Ð²Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°",
        "Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€"
    ]

    ESCALATION_RESPONSE = (
        "ÐŸÐ¾Ð½ÑÐ», ÑÐµÐ¹Ñ‡Ð°Ñ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ñƒ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°. "
        "ÐžÐ½ ÑÐ²ÑÐ¶ÐµÑ‚ÑÑ Ñ Ð²Ð°Ð¼Ð¸ Ð² Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐµÐµ Ð²Ñ€ÐµÐ¼Ñ."
    )

    SYSTEM_PROMPT = """Ð¢Ñ‹ â€” Ð²ÐµÐ¶Ð»Ð¸Ð²Ñ‹Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº Ð¿Ñ€Ð¾Ð´Ð°Ð²Ñ†Ð° Ð½Ð° Avito. 
ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¿Ð¾ÐºÑƒÐ¿Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¸Ð· Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹.

Ð’ÐÐ–ÐÐ«Ð• ÐŸÐ ÐÐ’Ð˜Ð›Ð:
1. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ñ… Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹
2. ÐÐ• Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ñ†ÐµÐ½Ñ‹, Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸, Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð¸Ð»Ð¸ ÑƒÑÐ»Ð¾Ð²Ð¸Ñ
3. Ð•ÑÐ»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð´Ð»Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° â€” Ñ‚Ð°Ðº Ð¸ ÑÐºÐ°Ð¶Ð¸
4. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ
5. Ð‘ÑƒÐ´ÑŒ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¼ Ð¸ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¼

ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° (Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ):
{context}

Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¸Ð· Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹:
{knowledge}

Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾ÐºÑƒÐ¿Ð°Ñ‚ÐµÐ»Ñ: {question}

ÐžÑ‚Ð²ÐµÑ‚:"""

    def __init__(self, api_key: str, model_name: str = "gemini-1.5-flash"):
        """Initialize answer policy.
        
        Args:
            api_key: Google Gemini API key
            model_name: Name of the Gemini model to use
        """
        self.api_key = api_key
        self.model_name = model_name
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    def needs_escalation(self, message: str) -> bool:
        """Check if message contains escalation keywords.
        
        Args:
            message: The customer message to check
            
        Returns:
            True if escalation is requested, False otherwise.
        """
        message_lower = message.lower()
        for keyword in self.ESCALATION_KEYWORDS:
            if keyword in message_lower:
                logger.info(f"Escalation keyword detected: '{keyword}'")
                return True
        return False

    def _format_context(self, messages: list[StoredMessage]) -> str:
        """Format chat history for the prompt.
        
        Args:
            messages: List of stored messages from chat history
            
        Returns:
            Formatted string with conversation context.
        """
        if not messages:
            return "ÐÐµÑ‚ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹."

        lines = []
        for msg in messages[-10:]:  # Last 10 messages for context
            sender = "Ð‘Ð¾Ñ‚" if msg.is_bot_message else "ÐŸÐ¾ÐºÑƒÐ¿Ð°Ñ‚ÐµÐ»ÑŒ"
            text = msg.text or "[Ð±ÐµÐ· Ñ‚ÐµÐºÑÑ‚Ð°]"
            lines.append(f"{sender}: {text}")

        return "\n".join(lines)

    def _format_knowledge(self, retrieval_result: RetrievalResult) -> str:
        """Format retrieved chunks for the prompt.
        
        Args:
            retrieval_result: Result from cascading retrieval
            
        Returns:
            Formatted string with knowledge base information.
        """
        if not retrieval_result.found or not retrieval_result.chunks:
            return "Ð ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°."

        lines = []
        for i, chunk in enumerate(retrieval_result.chunks, 1):
            source = chunk.source_file
            text = chunk.text[:500]  # Limit chunk size
            lines.append(f"[{i}] Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº: {source}\n{text}")

        return "\n\n".join(lines)

    def _extract_sources(self, retrieval_result: RetrievalResult) -> list[str]:
        """Extract unique source file names from retrieval result.
        
        Args:
            retrieval_result: Result from cascading retrieval
            
        Returns:
            List of unique source file names.
        """
        sources = set()
        for chunk in retrieval_result.chunks:
            if chunk.source_file and chunk.source_file != "unknown":
                sources.add(chunk.source_file)
        return list(sources)

    async def generate_answer(
        self,
        question: str,
        context: list[StoredMessage],
        retrieval_result: RetrievalResult
    ) -> AnswerResult:
        """Generate answer based on RAG results with anti-hallucination contract.
        
        Args:
            question: The customer's question
            context: Chat history for context
            retrieval_result: Result from cascading retrieval
            
        Returns:
            AnswerResult with answer, status, and sources.
        """
        # Check for escalation first
        if self.needs_escalation(question):
            logger.info("Escalation requested, returning escalation response")
            return AnswerResult(
                answer=self.ESCALATION_RESPONSE,
                found_status="ESCALATION",
                sources=[],
                is_escalation=True
            )

        # Anti-hallucination: if no results, return fallback
        if not retrieval_result.found or not retrieval_result.chunks:
            logger.info("No RAG results, returning fallback message")
            return AnswerResult(
                answer=self.FALLBACK_MESSAGE,
                found_status="NOT_FOUND",
                sources=[],
                is_escalation=False
            )

        # Generate answer using Gemini with grounded context
        try:
            prompt = self.SYSTEM_PROMPT.format(
                context=self._format_context(context),
                knowledge=self._format_knowledge(retrieval_result),
                question=question
            )

            response = await self.model.generate_content_async(
                prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.3,  # Lower temperature for factual responses
                    max_output_tokens=500
                )
            )

            answer = response.text.strip() if response.text else self.FALLBACK_MESSAGE
            sources = self._extract_sources(retrieval_result)

            # Validate answer is not empty
            if not answer:
                logger.warning("Empty answer generated, using fallback")
                return AnswerResult(
                    answer=self.FALLBACK_MESSAGE,
                    found_status="NOT_FOUND",
                    sources=[],
                    is_escalation=False
                )

            logger.info(
                f"Generated answer with {len(sources)} sources, "
                f"strategy={retrieval_result.search_strategy}"
            )

            return AnswerResult(
                answer=answer,
                found_status="FOUND",
                sources=sources,
                is_escalation=False
            )

        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            # On error, return fallback to avoid hallucination
            return AnswerResult(
                answer=self.FALLBACK_MESSAGE,
                found_status="NOT_FOUND",
                sources=[],
                is_escalation=False
            )
